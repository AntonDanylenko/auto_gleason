{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Gleason Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/danylenko/auto_gleason/env/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import openslide\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "# Torch packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torchvision.transforms import CenterCrop\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import math\n",
    "import time\n",
    "from IPython.display import Image, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Plotly for the interactive viewer\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Location of the training images\n",
    "\n",
    "DATA_PATH = '../../ganz/data/panda_dataset'\n",
    "\n",
    "# image and mask directories\n",
    "data_dir = f'{DATA_PATH}/train_images'\n",
    "mask_dir = f'{DATA_PATH}/train_label_masks'\n",
    "\n",
    "\n",
    "# Location of training labels\n",
    "train = pd.read_csv(f'{DATA_PATH}/train.csv').set_index('image_id')\n",
    "test = pd.read_csv(f'{DATA_PATH}/test.csv')\n",
    "submission = pd.read_csv(f'{DATA_PATH}/sample_submission.csv')\n",
    "\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "# # determine if we will be pinning memory during data loading\n",
    "# PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0005f7aaab2800f6170c399693a96917', '000920ad0b612851f8e01bcc880d9b3d', '0018ae58b01bdadc8e347995b69f99aa', '001c62abd11fa4b57bf7a6c603a11bb9', '001d865e65ef5d2579c190a0e0350d8f', '002a4db09dad406c85505a00fb6f6144', '003046e27c8ead3e3db155780dc5498e', '0032bfa835ce0f43a92ae0bbab6871cb', '003a91841da04a5a31f808fb5c21538a', '003d4dd6bd61221ebc0bfb9350db333f']\n"
     ]
    }
   ],
   "source": [
    "# all_train_img_names = [\n",
    "#     '07a7ef0ba3bb0d6564a73f4f3e1c2293',\n",
    "#     '037504061b9fba71ef6e24c48c6df44d',\n",
    "#     '035b1edd3d1aeeffc77ce5d248a01a53',\n",
    "#     '059cbf902c5e42972587c8d17d49efed',\n",
    "#     '06a0cbd8fd6320ef1aa6f19342af2e68',\n",
    "#     '06eda4a6faca84e84a781fee2d5f47e1',\n",
    "#     '0a4b7a7499ed55c71033cefb0765e93d',\n",
    "#     '0838c82917cd9af681df249264d2769c',\n",
    "#     '046b35ae95374bfb48cdca8d7c83233f',\n",
    "#     '074c3e01525681a275a42282cd21cbde',\n",
    "#     '05abe25c883d508ecc15b6e857e59f32',\n",
    "#     '05f4e9415af9fdabc19109c980daf5ad',\n",
    "#     '060121a06476ef401d8a21d6567dee6d',\n",
    "#     '068b0e3be4c35ea983f77accf8351cc8',\n",
    "#     '08f055372c7b8a7e1df97c6586542ac8'\n",
    "# ]\n",
    "\n",
    "all_train_img_names = list(train.index)\n",
    "print(all_train_img_names[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(slides): \n",
    "    f, ax = plt.subplots(5,3, figsize=(18,22))\n",
    "    for i, slide in enumerate(slides):\n",
    "        image = openslide.OpenSlide(os.path.join(data_dir, f'{slide}.tiff'))\n",
    "        # spacing = 1 / (float(image.properties['tiff.XResolution']) / 10000)\n",
    "        patch = image.read_region((1780,1950), 0, (256, 256))\n",
    "        ax[i//3, i%3].imshow(patch) \n",
    "        image.close()       \n",
    "        ax[i//3, i%3].axis('off')\n",
    "        \n",
    "        image_id = slide\n",
    "        data_provider = train.loc[slide, 'data_provider']\n",
    "        isup_grade = train.loc[slide, 'isup_grade']\n",
    "        gleason_score = train.loc[slide, 'gleason_score']\n",
    "        ax[i//3, i%3].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_images(all_train_img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_masks(slides): \n",
    "    f, ax = plt.subplots(5,3, figsize=(18,22))\n",
    "    for i, slide in enumerate(slides):\n",
    "        \n",
    "        mask = openslide.OpenSlide(os.path.join(mask_dir, f'{slide}_mask.tiff'))\n",
    "        mask_data = mask.read_region((0,0), mask.level_count - 1, mask.level_dimensions[-1])\n",
    "        cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n",
    "\n",
    "        ax[i//3, i%3].imshow(np.asarray(mask_data)[:,:,0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5) \n",
    "        mask.close()       \n",
    "        ax[i//3, i%3].axis('off')\n",
    "        \n",
    "        image_id = slide\n",
    "        data_provider = train.loc[slide, 'data_provider']\n",
    "        isup_grade = train.loc[slide, 'isup_grade']\n",
    "        gleason_score = train.loc[slide, 'gleason_score']\n",
    "        ax[i//3, i%3].set_title(f\"ID: {image_id}\\nSource: {data_provider} ISUP: {isup_grade} Gleason: {gleason_score}\")\n",
    "        f.tight_layout()\n",
    "        \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_masks(all_train_img_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of channels in the input, number of classes,\n",
    "# and number of levels in the U-Net model\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 1\n",
    "NUM_LEVELS = 3\n",
    "# initialize learning rate, number of epochs to train for, and the batch size\n",
    "INIT_LR = 0.001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 64\n",
    "# define the input image dimensions\n",
    "PATCH_WIDTH = 256\n",
    "PATCH_HEIGHT = 256\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# define the validation split\n",
    "VAL_SPLIT = 0.85\n",
    "\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"../output\"\n",
    "# define the path to the output serialized model, model training\n",
    "# plot, and testing image paths\n",
    "MODEL_PATH = f\"{BASE_OUTPUT}/unet_tgs_salt.pth\"\n",
    "PLOT_PATH = f\"{BASE_OUTPUT}/plot.png\"\n",
    "TEST_PATHS = f\"{BASE_OUTPUT}/test_paths.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_set(slides):\n",
    "  imgs = []\n",
    "  masks = []\n",
    "  for i, slide in enumerate(slides):\n",
    "    image = openslide.OpenSlide(os.path.join(data_dir, f'{slide}.tiff'))\n",
    "    mask = openslide.OpenSlide(os.path.join(mask_dir, f'{slide}_mask.tiff'))\n",
    "    # print(\"img level_count: \" + str(image.level_count))\n",
    "    # print(\"img level 0 dimension x: \" + str(image.dimensions[0]))\n",
    "    # print(\"img level 0 dimension y: \" + str(image.dimensions[1]))\n",
    "    # print(\"img level 1 dimension x: \" + str(image.level_dimensions[1][0]))\n",
    "    # print(\"img level 1 dimension y: \" + str(image.level_dimensions[1][1]))\n",
    "    # print(\"img level 2 dimension x: \" + str(image.level_dimensions[2][0]))\n",
    "    # print(\"img level 2 dimension y: \" + str(image.level_dimensions[2][1]))\n",
    "    # print(\"mask level 0 dimension x: \" + str(mask.dimensions[0]))\n",
    "    # print(\"mask level 0 dimension y: \" + str(mask.dimensions[1]))\n",
    "    max_x = image.dimensions[0] - (image.dimensions[0] % PATCH_WIDTH)\n",
    "    max_y = image.dimensions[1] - (image.dimensions[1] % PATCH_HEIGHT)\n",
    "    x = 0\n",
    "    while x < max_x:\n",
    "      y = 0\n",
    "      while y < max_y:\n",
    "        imgs.append(image.read_region((x,y), 0, (PATCH_WIDTH, PATCH_HEIGHT)))\n",
    "        masks.append(mask.read_region((x,y), 0, (PATCH_WIDTH, PATCH_HEIGHT)))\n",
    "        y+=PATCH_HEIGHT\n",
    "      x+=PATCH_WIDTH\n",
    "  return imgs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img level 0 dimension x: 10496\n",
      "img level 0 dimension y: 24832\n",
      "mask level 0 dimension x: 10496\n",
      "mask level 0 dimension y: 24832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([], [])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_image_set(all_train_img_names[1:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "  def __init__(self, wsi_names, pseudo_epoch_length: int = 1024):\n",
    "    self.wsi_names = wsi_names\n",
    "    self.pseudo_epoch_length = pseudo_epoch_length\n",
    "\n",
    "    # opens all slides and stores them in slide_dict\n",
    "    self.slide_dict = self.make_slide_dict(wsi_names=wsi_names)\n",
    "\n",
    "    # samples a list of patch coordinates and annotations \n",
    "    self.sample_dict = self.sample_coord_list(wsi_names=self.wsi_names,\n",
    "                                            pseudo_epoch_length=self.pseudo_epoch_length)\n",
    "\n",
    "  def make_slide_dict(self, wsi_names):\n",
    "    slide_dict = {}\n",
    "    for wsi_name in tqdm(wsi_names, total=len(wsi_names), desc='Make Slide Dict'):\n",
    "      if wsi_name not in slide_dict:\n",
    "        slide_dict[wsi_name] = {}\n",
    "        slide_dict[wsi_name]['slide'] = openslide.OpenSlide(os.path.join(data_dir, f'{wsi_name}.tiff'))\n",
    "        slide_dict[wsi_name]['mask'] = openslide.OpenSlide(os.path.join(mask_dir, f'{wsi_name}_mask.tiff'))\n",
    "        slide_dict[wsi_name]['size'] = slide_dict[wsi_name]['slide'].dimensions\n",
    "    return slide_dict\n",
    "\n",
    "  def sample_coord_list(self, wsi_names, pseudo_epoch_length):\n",
    "    # sample random coordinates\n",
    "    filenames, coords = self._sample_random_coords(pseudo_epoch_length)\n",
    "    \n",
    "    # bring everything in one dict\n",
    "    sample_dict = {}\n",
    "    for index, (filename, coord) in enumerate(zip(filenames, coords)):\n",
    "      sample_dict[index] = {'filename': filename, 'coordinates': coord}\n",
    "\n",
    "    return sample_dict\n",
    "\n",
    "  def _sample_random_coords(self, pseudo_epoch_length):\n",
    "    filenames = list(random.choice(self.wsi_names, size=pseudo_epoch_length, replace=True))\n",
    "    coords = []\n",
    "    for filenames in filenames:\n",
    "      width, height = self.slide_dict[filename]['size']\n",
    "      xy = list(random.randint(low=(0, 0), \n",
    "                              high=(width-PATCH_WIDTH, height-PATCH_HEIGHT),\n",
    "                              size=2))\n",
    "      coords.append(xy)\n",
    "    return filenames, coords\n",
    "\n",
    "  def __len__(self):\n",
    "    # return the number of total samples\n",
    "    return len(self.pseudo_epoch_length)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # grab the image from the current index\n",
    "    coords = self.sample_dict[index]['coordinates'].copy()\n",
    "    filename = self.sample_dict[index]['filename']\n",
    "\n",
    "    # load patch and mask\n",
    "    img = self.load_image(filename, coords, 'slide')\n",
    "    mask = self.load_image(filename, coords, 'mask')\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "  def load_image(self, filename, coords, type):\n",
    "    \"\"\"Loads an image patch from a slide and returns it as a numpy array.\"\"\"\n",
    "    slide = self.slide_dict[filename][type]\n",
    "    img = slide.read_region(coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0)\n",
    "    return np.asarray(img, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the convolution and RELU layers\n",
    "\t\tself.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "\t\tself.relu = ReLU()\n",
    "\t\tself.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "    \n",
    "\tdef forward(self, x):\n",
    "\t\t# apply CONV => RELU => CONV block to the inputs and return it\n",
    "\t\treturn self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class Encoder(Module):\n",
    "\tdef __init__(self, channels=(3, 16, 32, 64)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the encoder blocks and maxpooling layer\n",
    "\t\tself.encBlocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.pool = MaxPool2d(2)\n",
    "    \n",
    "\tdef forward(self, x):\n",
    "\t\t# initialize an empty list to store the intermediate outputs\n",
    "\t\tblockOutputs = []\n",
    "\t\t# loop through the encoder blocks\n",
    "\t\tfor block in self.encBlocks:\n",
    "\t\t\t# pass the inputs through the current encoder block, store\n",
    "\t\t\t# the outputs, and then apply maxpooling on the output\n",
    "\t\t\tx = block(x)\n",
    "\t\t\tblockOutputs.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\t\t# return the list containing the intermediate outputs\n",
    "\t\treturn blockOutputs\n",
    "\n",
    "class Decoder(Module):\n",
    "\tdef __init__(self, channels=(64, 32, 16)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the number of channels, upsampler blocks, and\n",
    "\t\t# decoder blocks\n",
    "\t\tself.channels = channels\n",
    "\t\tself.upconvs = ModuleList(\n",
    "\t\t\t[ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.dec_blocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\n",
    "\tdef forward(self, x, encFeatures):\n",
    "\t\t# loop through the number of channels\n",
    "\t\tfor i in range(len(self.channels) - 1):\n",
    "\t\t\t# pass the inputs through the upsampler blocks\n",
    "\t\t\tx = self.upconvs[i](x)\n",
    "\t\t\t# crop the current features from the encoder blocks,\n",
    "\t\t\t# concatenate them with the current upsampled features,\n",
    "\t\t\t# and pass the concatenated output through the current\n",
    "\t\t\t# decoder block\n",
    "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
    "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
    "\t\t\tx = self.dec_blocks[i](x)\n",
    "\t\t# return the final decoder output\n",
    "\t\treturn x\n",
    "    \n",
    "\tdef crop(self, encFeatures, x):\n",
    "\t\t# grab the dimensions of the inputs, and crop the encoder\n",
    "\t\t# features to match the dimensions\n",
    "\t\t(_, _, H, W) = x.shape\n",
    "\t\tencFeatures = CenterCrop([H, W])(encFeatures)\n",
    "\t\t# return the cropped features\n",
    "\t\treturn encFeatures\n",
    "\t\t\n",
    "class UNet(Module):\n",
    "\tdef __init__(self, encChannels=(3, 16, 32, 64),\n",
    "\t\t decChannels=(64, 32, 16),\n",
    "\t\t nbClasses=1, retainDim=True,\n",
    "\t\t outSize=(PATCH_HEIGHT,  PATCH_WIDTH)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the encoder and decoder\n",
    "\t\tself.encoder = Encoder(encChannels)\n",
    "\t\tself.decoder = Decoder(decChannels)\n",
    "\t\t# initialize the regression head and store the class variables\n",
    "\t\tself.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "\t\tself.retainDim = retainDim\n",
    "\t\tself.outSize = outSize\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# grab the features from the encoder\n",
    "\t\tencFeatures = self.encoder(x)\n",
    "\t\t# pass the encoder features through decoder making sure that\n",
    "\t\t# their dimensions are suited for concatenation\n",
    "\t\tdecFeatures = self.decoder(encFeatures[::-1][0],\n",
    "\t\t\tencFeatures[::-1][1:])\n",
    "\t\t# pass the decoder features through the regression head to\n",
    "\t\t# obtain the segmentation mask\n",
    "\t\tmap = self.head(decFeatures)\n",
    "\t\t# check to see if we are retaining the original output\n",
    "\t\t# dimensions and if so, then resize the output to match them\n",
    "\t\tif self.retainDim:\n",
    "\t\t\tmap = F.interpolate(map, self.outSize)\n",
    "\t\t# return the segmentation map\n",
    "\t\treturn map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8b36508994d132ac03c5effd255b6eaf', 'aca61df8852daa183fa8cbf30a2e8660', 'a4fe6a765a5d525015f97f2f967caa67', '18e2d22113d3bf1ef0575c90cb22fde6', '037f29b7d62479c7fb257903f401269a', '0fe439d806b88e850a6a69442d295530', '2e512e923950da10c0b4432d08ca0663', 'ef528624c588c2f9d04c79d400f1aacc', 'd4f83e0d9b46e89cd5e9da401f625bce', 'cc3d4d77585447466f27904725bd1d7b']\n",
      "['8a62c7e1f8070d642c7de7156dcf55bb', 'ce04c1a8d8577bb9620620c0a62edb15', '375771fed439d26b5374aae3f6b1d653', '0ec5bf6cbb0a9195b4db46173203a848', 'ac07fd6531d7536124ceab1990d5227d', 'f35124ad7e0c9d8537a8e57966c06445', 'e692db297233f27312315e8a5c75c32a', 'e3177c4430b780c973ef857a320a211d', '92d641eafe819ad90148027f5da9afd3', '5c3aa6266bfcbf2f9bf057a0a5e39520']\n"
     ]
    }
   ],
   "source": [
    "# all_train_imgs, all_train_masks = create_image_set(all_train_img_names)\n",
    "\n",
    "# partition the data into training and validation splits using 85% of\n",
    "# the data for training and the remaining 15% for validation\n",
    "split_size = math.floor(VAL_SPLIT*len(all_train_img_names))\n",
    "split = torch.utils.data.random_split(all_train_img_names,\n",
    "\t[split_size, len(all_train_img_names)-split_size], \n",
    "\tgenerator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# unpack the data split\n",
    "(train_img_names, val_img_names) = split\n",
    "train_img_names = list(train_img_names)\n",
    "val_img_names = list(val_img_names)\n",
    "# print(train_img_names[:10])\n",
    "# print(val_img_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 9023 wsi in the training set...\n",
      "[INFO] found 1593 wsi in the test set...\n"
     ]
    }
   ],
   "source": [
    "# create the train and validation datasets\n",
    "trainDS = SegmentationDataset(wsi_names=train_img_names, pseudo_epoch_length=1024)\n",
    "valDS = SegmentationDataset(wsi_names=val_img_names, pseudo_epoch_length=1024)\n",
    "print(f\"[INFO] found {len(trainDS)} samples in the training set...\")\n",
    "print(f\"[INFO] found {len(valDS)} samples in the validation set...\")\n",
    "\n",
    "# create the training and validation data loaders\n",
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "\tbatch_size=BATCH_SIZE, num_workers=4)\n",
    "valLoader = DataLoader(valDS, shuffle=False,\n",
    "\tbatch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [04:34<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/danylenko/auto_gleason/main.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000028vscode-remote?line=21'>22</a>\u001b[0m valSteps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000028vscode-remote?line=23'>24</a>\u001b[0m \u001b[39m# loop over the training set\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000028vscode-remote?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m (i, (x, y)) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainLoader):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000028vscode-remote?line=25'>26</a>\u001b[0m \t\u001b[39mfor\u001b[39;00m (x_patch, y_patch) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(x, y):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000028vscode-remote?line=26'>27</a>\u001b[0m \t\t\u001b[39m# send the input to the device\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000028vscode-remote?line=27'>28</a>\u001b[0m \t\t(x_patch, y_patch) \u001b[39m=\u001b[39m (x_patch\u001b[39m.\u001b[39mto(DEVICE), y_patch\u001b[39m.\u001b[39mto(DEVICE))\n",
      "File \u001b[0;32m~/auto_gleason/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/auto_gleason/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1207\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[1;32m   1206\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m-> 1207\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[1;32m   1208\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[1;32m   1210\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/auto_gleason/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1173\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1169\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1170\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1171\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1173\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[1;32m   1174\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[1;32m   1175\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/auto_gleason/env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1011\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_try_get_data\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m_utils\u001b[39m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m    999\u001b[0m     \u001b[39m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m     \u001b[39m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[39m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1009\u001b[0m     \u001b[39m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1011\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m   1012\u001b[0m         \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n\u001b[1;32m   1013\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1014\u001b[0m         \u001b[39m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m         \u001b[39m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m         \u001b[39m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/queues.py:107\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[39mif\u001b[39;00m block:\n\u001b[1;32m    106\u001b[0m     timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39mmonotonic()\n\u001b[0;32m--> 107\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout):\n\u001b[1;32m    108\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[1;32m    109\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    256\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_poll(timeout)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:424\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_poll\u001b[39m(\u001b[39mself\u001b[39m, timeout):\n\u001b[0;32m--> 424\u001b[0m     r \u001b[39m=\u001b[39m wait([\u001b[39mself\u001b[39;49m], timeout)\n\u001b[1;32m    425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:936\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m         timeout \u001b[39m=\u001b[39m deadline \u001b[39m-\u001b[39m time\u001b[39m.\u001b[39;49mmonotonic()\n\u001b[1;32m    937\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    938\u001b[0m             \u001b[39mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# initialize our UNet model\n",
    "unet = UNet().to(DEVICE)\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = BCEWithLogitsLoss()\n",
    "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDS) // BATCH_SIZE\n",
    "valSteps = len(valDS) // BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "\tunet.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\n",
    "\t# loop over the training set\n",
    "\tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "    # send the input to the device\n",
    "    (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "    # perform a forward pass and calculate the training loss\n",
    "    pred = unet(x)\n",
    "    loss = lossFunc(pred, y)\n",
    "    # first, zero out any previously accumulated gradients, then\n",
    "    # perform backpropagation, and then update model parameters\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    # add the loss to the total training loss so far\n",
    "    totalTrainLoss += loss\n",
    "    \n",
    "\t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tunet.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in valLoader:\n",
    "      # send the input to the device\n",
    "      (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "      # make the predictions and calculate the validation loss\n",
    "      pred = unet(x)\n",
    "      totalValLoss += lossFunc(pred, y)\n",
    "\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Val loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgValLoss))\n",
    "\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(PLOT_PATH)\n",
    "# serialize the model to disk\n",
    "torch.save(unet, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_plot(origImage, origMask, predMask):\n",
    "\t# initialize our figure\n",
    "\tfigure, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 10))\n",
    "\t# plot the original image, its mask, and the predicted mask\n",
    "\tax[0].imshow(origImage)\n",
    "\tax[1].imshow(origMask)\n",
    "\tax[2].imshow(predMask)\n",
    "\t# set the titles of the subplots\n",
    "\tax[0].set_title(\"Image\")\n",
    "\tax[1].set_title(\"Original Mask\")\n",
    "\tax[2].set_title(\"Predicted Mask\")\n",
    "\t# set the layout of the figure and display it\n",
    "\tfigure.tight_layout()\n",
    "\tfigure.show()\n",
    "\n",
    "def make_predictions(model, test_image, test_mask):\n",
    "\t# set model to evaluation mode\n",
    "\tmodel.eval()\n",
    "\t# turn off gradient tracking\n",
    "\twith torch.no_grad():\n",
    "\t\t# resize the image and make a copy of it for visualization\n",
    "\t\t# test_image = cv2.resize(test_image, (PATCH_WIDTH, PATCH_HEIGHT))\n",
    "\t\torig = test_image.copy()\n",
    "\n",
    "  \t# make the channel axis to be the leading one, add a batch\n",
    "\t\t# dimension, create a PyTorch tensor, and flash it to the\n",
    "\t\t# current device\n",
    "\t\ttest_image = np.transpose(test_image, (2, 0, 1))\n",
    "\t\ttest_image = np.expand_dims(test_image, 0)\n",
    "\t\ttest_image = torch.from_numpy(test_image).to(DEVICE)\n",
    "\t\t# make the prediction, pass the results through the sigmoid\n",
    "\t\t# function, and convert the result to a NumPy array\n",
    "\t\tpredMask = model(test_image).squeeze()\n",
    "\t\tpredMask = torch.sigmoid(predMask)\n",
    "\t\tpredMask = predMask.cpu().numpy()\n",
    "\t\t# filter out the weak predictions and convert them to integers\n",
    "\t\tpredMask = (predMask > THRESHOLD) * 255\n",
    "\t\tpredMask = predMask.astype(np.uint8)\n",
    "\t\t# prepare a plot for visualization\n",
    "\t\tprepare_plot(orig, gtMask, predMask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get test_img ids\n",
    "# test_img_names = list(test.index)\n",
    "# print(test_img_names[:10])\n",
    "\n",
    "# random_test_img_names = random.choice(test_img_names, size=10)\n",
    "\n",
    "# # # create the train and validation datasets\n",
    "# # trainDS = SegmentationDataset(wsi_names=random_test_img_names, pseudo_epoch_length=1024)\n",
    "# # valDS = SegmentationDataset(wsi_names=val_img_names, pseudo_epoch_length=1024)\n",
    "# # print(f\"[INFO] found {len(trainDS)} samples in the training set...\")\n",
    "# # print(f\"[INFO] found {len(valDS)} samples in the validation set...\")\n",
    "\n",
    "# # create the training and validation data loaders\n",
    "# trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "# \tbatch_size=BATCH_SIZE, num_workers=4)\n",
    "# valLoader = DataLoader(valDS, shuffle=False,\n",
    "# \tbatch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "# # load the image paths in our testing file and randomly select 10 image paths\n",
    "# print(\"[INFO] loading up test images...\")\n",
    "# random_test_img_names = random.choice(test_img_names, size=10)\n",
    "# test_imgs, test_masks = create_image_set(random_test_img_names)\n",
    "\n",
    "# # load our model from disk and flash it to the current device\n",
    "# print(\"[INFO] load up model...\")\n",
    "# unet = torch.load(MODEL_PATH).to(DEVICE)\n",
    "\n",
    "# # iterate over the randomly selected test image paths\n",
    "# for test_img, test_mask in zip(test_imgs, test_masks):\n",
    "# \t# make predictions and visualize the results\n",
    "# \tmake_predictions(unet, test_img, test_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de4631684bd2c882b6b797b2a828f7be340f598e9083880a321a8ea7c6fac874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
