{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Gleason Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openslide\n",
    "\n",
    "# Torch packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from focal_loss import FocalLoss\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "\n",
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Globals and Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of channels in the input, number of classes,\n",
    "# and number of levels in the U-Net model\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 6\n",
    "NUM_LEVELS = 3\n",
    "# initialize learning rate, number of epochs to train for, and the batch size\n",
    "INIT_LR = 0.001\n",
    "NUM_EPOCHS = 40\n",
    "BATCH_SIZE = 32\n",
    "NUM_PSEUDO_EPOCHS = 1024\n",
    "# define the input image dimensions\n",
    "PATCH_WIDTH = 256 #256\n",
    "PATCH_HEIGHT = 256\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# define the validation split\n",
    "VAL_SPLIT = 0.85\n",
    "\n",
    "# define the test split\n",
    "TEST_SPLIT = 0.95\n",
    "\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"./output\"\n",
    "# define the path to the output serialized model, model training\n",
    "# plot, and testing image paths\n",
    "MODEL_PATH = f\"{BASE_OUTPUT}/unet_tgs_salt.pth\"\n",
    "PLOT_PATH = f\"{BASE_OUTPUT}/plot.png\"\n",
    "TEST_PATHS = f\"{BASE_OUTPUT}/test_paths.txt\"\n",
    "\n",
    "# Location of the training images\n",
    "DATA_PATH = '../../ganz/data/panda_dataset'\n",
    "\n",
    "# image and mask directories\n",
    "data_dir = f'{DATA_PATH}/train_images'\n",
    "mask_dir = f'{DATA_PATH}/train_label_masks'\n",
    "\n",
    "# Location of training labels\n",
    "train = pd.read_csv(f'{DATA_PATH}/train.csv').set_index('image_id')\n",
    "test = pd.read_csv(f'{DATA_PATH}/test.csv').set_index('image_id')\n",
    "submission = pd.read_csv(f'{DATA_PATH}/sample_submission.csv')\n",
    "\n",
    "# # Create training image name list\n",
    "# all_train_img_names = list(train.index)[:12]\n",
    "\n",
    "# Get mask thumbnail dictionary\n",
    "thumbnail_filename = \"./data/thumbnails_\" + str(PATCH_WIDTH) + \"x\" + str(PATCH_HEIGHT) + \".p\"\n",
    "with open(thumbnail_filename, \"rb\") as fp:\n",
    "    thumbnails_dict = pickle.load(fp)\n",
    "\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(DEVICE)\n",
    "# # determine if we will be pinning memory during data loading\n",
    "# PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
    "\n",
    "# color map for printing masks\n",
    "cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n",
    "\n",
    "# Sample images\n",
    "# all_train_img_names = [\n",
    "#     '07a7ef0ba3bb0d6564a73f4f3e1c2293',\n",
    "#     '037504061b9fba71ef6e24c48c6df44d',\n",
    "#     '035b1edd3d1aeeffc77ce5d248a01a53',\n",
    "#     '059cbf902c5e42972587c8d17d49efed',\n",
    "#     '06a0cbd8fd6320ef1aa6f19342af2e68',\n",
    "#     '06eda4a6faca84e84a781fee2d5f47e1',\n",
    "#     '0a4b7a7499ed55c71033cefb0765e93d',\n",
    "#     '0838c82917cd9af681df249264d2769c',\n",
    "#     '046b35ae95374bfb48cdca8d7c83233f',\n",
    "#     '074c3e01525681a275a42282cd21cbde',\n",
    "#     '05abe25c883d508ecc15b6e857e59f32',\n",
    "#     '05f4e9415af9fdabc19109c980daf5ad',\n",
    "#     '060121a06476ef401d8a21d6567dee6d',\n",
    "#     '068b0e3be4c35ea983f77accf8351cc8',\n",
    "#     '08f055372c7b8a7e1df97c6586542ac8'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "  def __init__(self, \n",
    "              wsi_names, \n",
    "              mask_thumbnails,\n",
    "              pseudo_epoch_length: int = 1024, \n",
    "              transformations = None):\n",
    "    self.wsi_names = wsi_names\n",
    "    self.mask_thumbnails = mask_thumbnails\n",
    "    self.pseudo_epoch_length = pseudo_epoch_length\n",
    "\n",
    "    # opens all slides and stores them in slide_dict\n",
    "    self.slide_dict = self.make_slide_dict(wsi_names=self.wsi_names)\n",
    "\n",
    "    # samples a list of patch coordinates and annotations \n",
    "    self.sample_dict = self.sample_coord_list(pseudo_epoch_length=self.pseudo_epoch_length)\n",
    "\n",
    "    if transformations is not None:\n",
    "      self.transformations = transformations\n",
    "    else:\n",
    "      self.transformations = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "  def make_slide_dict(self, wsi_names):\n",
    "    slide_dict = {}\n",
    "    bad_samples = []\n",
    "    for wsi_name in tqdm(wsi_names, total=len(wsi_names), desc='Make Slide Dict'):\n",
    "      if wsi_name not in slide_dict:\n",
    "        slide_path = os.path.join(data_dir, f'{wsi_name}.tiff')\n",
    "        mask_path = os.path.join(mask_dir, f'{wsi_name}_mask.tiff')\n",
    "        if os.path.exists(slide_path) and os.path.exists(mask_path):\n",
    "          slide_dict[wsi_name] = {}\n",
    "          slide_dict[wsi_name]['slide'] = openslide.OpenSlide(slide_path)\n",
    "          slide_dict[wsi_name]['mask'] = openslide.OpenSlide(mask_path)\n",
    "          slide_dict[wsi_name]['size'] = slide_dict[wsi_name]['slide'].dimensions\n",
    "        else:\n",
    "          bad_samples.append(wsi_name)\n",
    "    # print(bad_samples)\n",
    "    # print(len(bad_samples))\n",
    "    for wsi_name in bad_samples:\n",
    "      self.wsi_names.remove(wsi_name)\n",
    "    return slide_dict\n",
    "\n",
    "  def sample_coord_list(self, pseudo_epoch_length):\n",
    "    # # sample random coordinates\n",
    "    # filenames, coords = self._sample_random_coords(pseudo_epoch_length)\n",
    "\n",
    "    # sample nonempty coordinates\n",
    "    filenames, coords = self._sample_nonempty_coords(pseudo_epoch_length)\n",
    "    \n",
    "    # bring everything in one dict\n",
    "    sample_dict = {}\n",
    "    for index, (filename, coord) in enumerate(zip(filenames, coords)):\n",
    "      sample_dict[index] = {'filename': filename, 'coordinates': coord}\n",
    "\n",
    "    return sample_dict\n",
    "\n",
    "  def _sample_nonempty_coords(self, pseudo_epoch_length):\n",
    "    filenames = []\n",
    "    coords = []\n",
    "    for i in range(pseudo_epoch_length):\n",
    "\n",
    "      # Select random file, it's mask thumbnail, and get all non-empty coordinates\n",
    "      filename = random.choice(self.wsi_names, size=1)[0]\n",
    "      # mask_slide = self.slide_dict[filename]['mask']\n",
    "      width, height = self.slide_dict[filename]['size']\n",
    "      mask_thumbnail = self.mask_thumbnails[filename]\n",
    "      indices = np.transpose(np.where(mask_thumbnail>0))\n",
    "      # print(indices.size)\n",
    "      while (indices.size==0):\n",
    "        filename = random.choice(self.wsi_names, size=1)[0]\n",
    "        # mask_slide = self.slide_dict[filename]['mask']\n",
    "        width, height = self.slide_dict[filename]['size']\n",
    "        mask_thumbnail = self.mask_thumbnails[filename]\n",
    "        indices = np.transpose(np.where(mask_thumbnail>0))\n",
    "      # print(indices.size)\n",
    "      # plt.figure(figsize=(20,20))\n",
    "      # plt.imshow(mask_thumbnail)\n",
    "      # plt.show()\n",
    "\n",
    "      # Pick random index and invert coordinates from (y,x) to (x,y)\n",
    "      rand_index = random.randint(len(indices))\n",
    "      coord = indices[rand_index][::-1]\n",
    "      # print(mask_thumbnail[coord[1]][coord[0]])\n",
    "\n",
    "      # Scale coord to wsi size and add a little randomness\n",
    "      coord[0] = coord[0]*PATCH_WIDTH + random.randint(low=-PATCH_WIDTH//8,\n",
    "                                                        high=PATCH_WIDTH//8)\n",
    "      if coord[0]<0: coord[0]=0\n",
    "      if coord[0]>width-PATCH_WIDTH: coord[0]=width-PATCH_WIDTH\n",
    "      coord[1] = coord[1]*PATCH_HEIGHT + random.randint(low=-PATCH_HEIGHT//8,\n",
    "                                                        high=PATCH_HEIGHT//8)\n",
    "      if coord[1]<0: coord[1]=0\n",
    "      if coord[1]>height-PATCH_HEIGHT: coord[1]=height-PATCH_HEIGHT\n",
    "\n",
    "      # coord[0] = coord[0]*PATCH_WIDTH\n",
    "      # coord[1] = coord[1]*PATCH_HEIGHT\n",
    "\n",
    "      # print(\"coord: \" + str(coord))\n",
    "      # print(\"width: \" + str(width) + \", height: \" + str(height))\n",
    "\n",
    "      filenames.append(filename)\n",
    "      coords.append(coord)\n",
    "    return filenames, coords\n",
    "\n",
    "  # def _sample_random_coords(self, pseudo_epoch_length):\n",
    "  #   filenames = list(random.choice(self.wsi_names, size=pseudo_epoch_length, replace=True))\n",
    "  #   coords = []\n",
    "  #   for filename in filenames: \n",
    "  #     width, height = self.slide_dict[filename]['size']\n",
    "  #     xy = list(random.randint(low=(0, 0), \n",
    "  #                             high=(width-PATCH_WIDTH, height-PATCH_HEIGHT),\n",
    "  #                             size=2))\n",
    "  #     coords.append(xy)\n",
    "  #   return filenames, coords\n",
    "\n",
    "  def __len__(self):\n",
    "    # return the number of total samples\n",
    "    return self.pseudo_epoch_length\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # grab the image from the current index\n",
    "    coords = self.sample_dict[index]['coordinates'].copy()\n",
    "    filename = self.sample_dict[index]['filename']\n",
    "\n",
    "    # load patch and mask\n",
    "    img, mask = self.load_image(filename, coords)\n",
    "\n",
    "    img = self.transformations(img)\n",
    "    # mask = self.transformations(mask)\n",
    "    # mask = torch.as_tensor(mask) #.permute(1, 2, 0)\n",
    "\n",
    "    # # Split mask into binary masks for each class\n",
    "    # channels = np.tile(mask, (NUM_CLASSES, 1, 1))\n",
    "    # for (i, channel) in enumerate(channels):\n",
    "    #     channel = torch.as_tensor(channel).type(dtype=torch.float32)\n",
    "    #     channels[i] = torch.tensor(1.0).where(channel==i+1, torch.tensor(0.0))\n",
    "    # mask = channels\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "  def load_image(self, filename, coords):\n",
    "    \"\"\"Loads an image and corresponding mask patch from a slide and returns it as a numpy array.\"\"\"\n",
    "    slide = self.slide_dict[filename]['slide']\n",
    "    patch = slide.read_region(coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0).convert('RGB')\n",
    "    mask_slide = self.slide_dict[filename]['mask']\n",
    "    mask_patch = mask_slide.read_region(coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0).convert('RGB')\n",
    "    # print(mask_patch.size)\n",
    "    # print(np.mean(mask_patch))\n",
    "    # print(np.asarray(mask_patch, dtype=np.uint8))\n",
    "    return np.asarray(patch, dtype=np.uint8), np.asarray(mask_patch, dtype=np.uint8)[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the convolution and RELU layers\n",
    "\t\tself.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "\t\tself.relu = ReLU()\n",
    "\t\tself.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "    \n",
    "\tdef forward(self, x):\n",
    "\t\t# apply CONV => RELU => CONV block to the inputs and return it\n",
    "\t\treturn self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class Encoder(Module):\n",
    "\tdef __init__(self, channels=(3, 16, 32, 64)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the encoder blocks and maxpooling layer\n",
    "\t\tself.encBlocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.pool = MaxPool2d(2)\n",
    "    \n",
    "\tdef forward(self, x):\n",
    "\t\t# initialize an empty list to store the intermediate outputs\n",
    "\t\tblockOutputs = []\n",
    "\t\t# loop through the encoder blocks\n",
    "\t\tfor block in self.encBlocks:\n",
    "\t\t\t# pass the inputs through the current encoder block, store\n",
    "\t\t\t# the outputs, and then apply maxpooling on the output\n",
    "\t\t\tx = block(x)\n",
    "\t\t\tblockOutputs.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\t\t# return the list containing the intermediate outputs\n",
    "\t\treturn blockOutputs\n",
    "\n",
    "class Decoder(Module):\n",
    "\tdef __init__(self, channels=(64, 32, 16)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the number of channels, upsampler blocks, and\n",
    "\t\t# decoder blocks\n",
    "\t\tself.channels = channels\n",
    "\t\tself.upconvs = ModuleList(\n",
    "\t\t\t[ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.dec_blocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\n",
    "\tdef forward(self, x, encFeatures):\n",
    "\t\t# loop through the number of channels\n",
    "\t\tfor i in range(len(self.channels) - 1):\n",
    "\t\t\t# pass the inputs through the upsampler blocks\n",
    "\t\t\tx = self.upconvs[i](x)\n",
    "\t\t\t# crop the current features from the encoder blocks,\n",
    "\t\t\t# concatenate them with the current upsampled features,\n",
    "\t\t\t# and pass the concatenated output through the current\n",
    "\t\t\t# decoder block\n",
    "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
    "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
    "\t\t\tx = self.dec_blocks[i](x)\n",
    "\t\t# return the final decoder output\n",
    "\t\treturn x\n",
    "    \n",
    "\tdef crop(self, encFeatures, x):\n",
    "\t\t# grab the dimensions of the inputs, and crop the encoder\n",
    "\t\t# features to match the dimensions\n",
    "\t\t# print(x.shape)\n",
    "\t\t(_, _, H, W) = x.shape\n",
    "\t\t# H = x.shape[-2]\n",
    "\t\t# W = x.shape[-1]\n",
    "\t\tencFeatures = transforms.CenterCrop([H, W])(encFeatures)\n",
    "\t\t# return the cropped features\n",
    "\t\treturn encFeatures\n",
    "\t\t\n",
    "class UNet(Module):\n",
    "\tdef __init__(self, encChannels=(3, 16, 32, 64),\n",
    "\t\t decChannels=(64, 32, 16),\n",
    "\t\t nbClasses=NUM_CLASSES, retainDim=True,\n",
    "\t\t outSize=(PATCH_WIDTH, PATCH_HEIGHT)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the encoder and decoder\n",
    "\t\tself.encoder = Encoder(encChannels)\n",
    "\t\tself.decoder = Decoder(decChannels)\n",
    "\t\t# initialize the regression head and store the class variables\n",
    "\t\tself.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "\t\tself.retainDim = retainDim\n",
    "\t\tself.outSize = outSize\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# grab the features from the encoder\n",
    "\t\tencFeatures = self.encoder(x)\n",
    "\t\t# pass the encoder features through decoder making sure that\n",
    "\t\t# their dimensions are suited for concatenation\n",
    "\t\tdecFeatures = self.decoder(encFeatures[::-1][0],\n",
    "\t\t\tencFeatures[::-1][1:])\n",
    "\t\t# pass the decoder features through the regression head to\n",
    "\t\t# obtain the segmentation mask\n",
    "\t\tmap = self.head(decFeatures)\n",
    "\t\t# check to see if we are retaining the original output\n",
    "\t\t# dimensions and if so, then resize the output to match them\n",
    "\t\tif self.retainDim:\n",
    "\t\t\tmap = F.interpolate(map, self.outSize)\n",
    "\t\t# return the segmentation map\n",
    "\t\treturn map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make Slide Dict: 100%|██████████| 4166/4166 [00:03<00:00, 1155.87it/s]\n",
      "Make Slide Dict: 100%|██████████| 736/736 [00:00<00:00, 1203.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 1024 samples in the training set...\n",
      "[INFO] found 1024 samples in the validation set...\n"
     ]
    }
   ],
   "source": [
    "# Take only radboud slides\n",
    "radboud = train.loc[train[\"data_provider\"]==\"radboud\"]\n",
    "\n",
    "# Take only wsi names\n",
    "all__img_names = list(radboud.index)\n",
    "\n",
    "# partition the data into training and testing splits using 95% of\n",
    "# the data for training and the remaining 5% for testing\n",
    "test_split_size = math.floor(TEST_SPLIT*len(all__img_names))\n",
    "test_split = torch.utils.data.random_split(all__img_names,\n",
    "\t\t\t\t\t\t\t\t\t[test_split_size, len(all__img_names)-test_split_size], \n",
    "\t\t\t\t\t\t\t\t\tgenerator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# unpack the data split\n",
    "(all_train_img_names, test_img_names) = test_split\n",
    "all_train_img_names = list(all_train_img_names)\n",
    "test_img_names = list(test_img_names)\n",
    "\n",
    "# partition the data into training and validation splits using 85% of\n",
    "# the data for training and the remaining 15% for validation\n",
    "split_size = math.floor(VAL_SPLIT*len(all_train_img_names))\n",
    "split = torch.utils.data.random_split(all_train_img_names,\n",
    "\t\t\t\t\t\t\t\t\t[split_size, len(all_train_img_names)-split_size], \n",
    "\t\t\t\t\t\t\t\t\tgenerator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# unpack the data split\n",
    "(train_img_names, val_img_names) = split\n",
    "train_img_names = list(train_img_names)\n",
    "val_img_names = list(val_img_names)\n",
    "# print(train_img_names[:10])\n",
    "# print(val_img_names[:10])\n",
    "\n",
    "# create the train and validation datasets\n",
    "trainDS = SegmentationDataset(wsi_names=train_img_names, mask_thumbnails=thumbnails_dict, pseudo_epoch_length=NUM_PSEUDO_EPOCHS)\n",
    "valDS = SegmentationDataset(wsi_names=val_img_names, mask_thumbnails=thumbnails_dict, pseudo_epoch_length=NUM_PSEUDO_EPOCHS)\n",
    "print(f\"[INFO] found {len(trainDS)} samples in the training set...\")\n",
    "print(f\"[INFO] found {len(valDS)} samples in the validation set...\")\n",
    "\n",
    "# create the training and validation data loaders\n",
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "\tbatch_size=BATCH_SIZE, num_workers=4)\n",
    "valLoader = DataLoader(valDS, shuffle=False,\n",
    "\tbatch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "#     in_channels=3, out_channels=1, init_features=32, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our UNet model\n",
    "unet = UNet().to(DEVICE)\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = FocalLoss(0.25) #dice loss, focal loss\n",
    "# lossFunc = BCEWithLogitsLoss()\n",
    "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDS) // BATCH_SIZE\n",
    "valSteps = len(valDS) // BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"val_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize slide patches and masks\n",
    "\n",
    "# rows = 20\n",
    "\n",
    "# all_figure, all_ax = plt.subplots(nrows=rows, ncols=2, figsize=(30, 30))\n",
    "\n",
    "# for e in tqdm(range(rows)):\n",
    "# \t# loop over the training set\n",
    "# \tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "# \t\tif i<1:\n",
    "# \t\t\tall_ax[e,0].imshow(y[0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "# \t\t\tall_ax[e,1].imshow(torch.as_tensor(x[0]).permute(1, 2, 0), cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\n",
    "# all_figure.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compressMask(channels):\n",
    "#     mask = np.zeros((PATCH_WIDTH, PATCH_HEIGHT))\n",
    "#     for (i, channel) in enumerate(channels):\n",
    "#         mask += channel*(i+1)\n",
    "#     return mask\n",
    "\n",
    "# def compressPredMask(channels):\n",
    "#     mask = np.zeros((PATCH_WIDTH, PATCH_HEIGHT))\n",
    "#     for (i, channel) in enumerate(channels):\n",
    "#         mask += (0-np.rint(channel.numpy()/256))*(i+1)\n",
    "#     return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:10<07:08, 10.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/40\n",
      "Train loss: 0.178198, Val loss: 0.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:21<06:50, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/40\n",
      "Train loss: 0.134229, Val loss: 0.1347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:32<06:36, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 3/40\n",
      "Train loss: 0.131542, Val loss: 0.1341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:42<06:25, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 4/40\n",
      "Train loss: 0.130986, Val loss: 0.1336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:53<06:14, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 5/40\n",
      "Train loss: 0.130695, Val loss: 0.1350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [01:04<06:03, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 6/40\n",
      "Train loss: 0.129450, Val loss: 0.1315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [01:15<05:52, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 7/40\n",
      "Train loss: 0.128111, Val loss: 0.1318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [01:25<05:41, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 8/40\n",
      "Train loss: 0.125291, Val loss: 0.1257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [01:36<05:31, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 9/40\n",
      "Train loss: 0.117789, Val loss: 0.1105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [01:47<05:20, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 10/40\n",
      "Train loss: 0.109202, Val loss: 0.1065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [01:57<05:09, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 11/40\n",
      "Train loss: 0.107107, Val loss: 0.1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [02:08<04:59, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 12/40\n",
      "Train loss: 0.103560, Val loss: 0.1005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [02:19<04:48, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 13/40\n",
      "Train loss: 0.104121, Val loss: 0.1017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [02:29<04:37, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 14/40\n",
      "Train loss: 0.102652, Val loss: 0.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [02:40<04:26, 10.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 15/40\n",
      "Train loss: 0.101785, Val loss: 0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [02:51<04:15, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 16/40\n",
      "Train loss: 0.098338, Val loss: 0.0978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [03:01<04:06, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 17/40\n",
      "Train loss: 0.099543, Val loss: 0.0980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [03:12<03:55, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 18/40\n",
      "Train loss: 0.100674, Val loss: 0.0949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [03:23<03:44, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 19/40\n",
      "Train loss: 0.097483, Val loss: 0.0947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [03:33<03:33, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 20/40\n",
      "Train loss: 0.093452, Val loss: 0.0917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [03:44<03:23, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 21/40\n",
      "Train loss: 0.094256, Val loss: 0.0967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [03:55<03:12, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 22/40\n",
      "Train loss: 0.091536, Val loss: 0.0918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [04:06<03:02, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 23/40\n",
      "Train loss: 0.091590, Val loss: 0.0877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [04:16<02:51, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 24/40\n",
      "Train loss: 0.089655, Val loss: 0.0881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [04:27<02:40, 10.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 25/40\n",
      "Train loss: 0.088941, Val loss: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [04:38<02:30, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 26/40\n",
      "Train loss: 0.089230, Val loss: 0.0907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [04:49<02:19, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 27/40\n",
      "Train loss: 0.088253, Val loss: 0.0858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [04:59<02:09, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 28/40\n",
      "Train loss: 0.090683, Val loss: 0.0865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [05:10<01:58, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 29/40\n",
      "Train loss: 0.089866, Val loss: 0.0932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [05:21<01:47, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 30/40\n",
      "Train loss: 0.090401, Val loss: 0.0876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [05:32<01:36, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 31/40\n",
      "Train loss: 0.086405, Val loss: 0.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [05:42<01:25, 10.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 32/40\n",
      "Train loss: 0.086918, Val loss: 0.0860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [05:53<01:15, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 33/40\n",
      "Train loss: 0.086681, Val loss: 0.0849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [06:04<01:04, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 34/40\n",
      "Train loss: 0.086322, Val loss: 0.0873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [06:14<00:53, 10.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 35/40\n",
      "Train loss: 0.086265, Val loss: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [06:25<00:42, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 36/40\n",
      "Train loss: 0.086728, Val loss: 0.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [06:36<00:32, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 37/40\n",
      "Train loss: 0.084289, Val loss: 0.0838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [06:46<00:21, 10.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 38/40\n",
      "Train loss: 0.085717, Val loss: 0.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [06:57<00:10, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 39/40\n",
      "Train loss: 0.085754, Val loss: 0.0858\n"
     ]
    }
   ],
   "source": [
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "\tunet.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\n",
    "\t# loop over the training set\n",
    "\tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "\t\t# if i<1:\n",
    "\t\t# \tall_figure, all_ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 30))\n",
    "\t\t# \trealMask = y[0].numpy()\n",
    "\t\t# \tall_ax[1].imshow(realMask, cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\t\t# \tall_ax[0].imshow(torch.as_tensor(x[0]).permute(1, 2, 0), cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\tpred = unet(x).squeeze() #.type(dtype=torch.uint8)\n",
    "\t\ty = y.squeeze()\n",
    "\t\ty = y.type(dtype=torch.long)\n",
    "\t\t# print(f'preds {pred.shape}')\n",
    "\t\t# print(f'y shape: {y.shape}')\n",
    "\t\tloss = lossFunc(pred, y)\n",
    "\t\t# print(f'loss: {loss}')\n",
    "\t\t# print(f'loss shape: {loss.shape}')\n",
    "\t\t# first, zero out any previously accumulated gradients, then\n",
    "\t\t# perform backpropagation, and then update model parameters\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# if i<1:\n",
    "\t\t# \tout = torch.argmax(pred[0], dim=0)\n",
    "\t\t# \tall_ax[2].imshow(out.cpu().detach().numpy(), cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\t\t# \tall_figure.tight_layout()\n",
    "\t\t# \tplt.show()\n",
    "\t\t# add the loss to the total training loss so far\n",
    "\t\ttotalTrainLoss += loss\n",
    "    \n",
    "\t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tunet.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in valLoader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = unet(x).squeeze() #.type(dtype=torch.uint8)\n",
    "\t\t\ty = y.squeeze()\n",
    "\t\t\ty = y.type(dtype=torch.long)\n",
    "\t\t\ttotalValLoss += lossFunc(pred, y)\n",
    "\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Val loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgValLoss))\n",
    "\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(PLOT_PATH)\n",
    "# serialize the model to disk\n",
    "# torch.save(unet, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "\n",
    "test_img_name = test_img_names[1]\n",
    "# print(test_img_name)\n",
    "\n",
    "test_slide_path = os.path.join(data_dir, f'{test_img_name}.tiff')\n",
    "test_slide = openslide.OpenSlide(test_slide_path)\n",
    "test_slide_size = test_slide.dimensions\n",
    "# print(test_slide_size)\n",
    "thumbnail = test_slide.get_thumbnail((test_slide_size[0]/PATCH_WIDTH, test_slide_size[1]/PATCH_HEIGHT)).convert('RGB')\n",
    "ax[0].imshow(thumbnail)\n",
    "\n",
    "test_coords = [57*PATCH_WIDTH,28*PATCH_HEIGHT]\n",
    "test_patch = test_slide.read_region(test_coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0).convert('RGB')\n",
    "test_patch = np.asarray(test_patch, dtype=np.uint8)\n",
    "ax[1].imshow(test_patch)\n",
    "\n",
    "test_mask_path = os.path.join(mask_dir, f'{test_img_name}_mask.tiff')\n",
    "test_mask = openslide.OpenSlide(test_mask_path)\n",
    "test_mask_patch = test_mask.read_region(test_coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0).convert('RGB')\n",
    "test_mask_patch = np.asarray(test_mask_patch, dtype=np.uint8)[:,:,0]\n",
    "ax[2].imshow(test_mask_patch, cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\n",
    "unet.eval()\n",
    "with torch.no_grad():\n",
    "    # transformations = transforms.Compose([transforms.ToTensor()])\n",
    "    # # print(test_patch.shape)\n",
    "    test_patch = np.transpose(test_patch, (2,0,1))\n",
    "    # test_patch = np.expand_dims(test_patch, 0)\n",
    "    # test_patch = transformations(test_patch)\n",
    "    # print(test_patch.shape)\n",
    "    # Split mask into binary masks for each class\n",
    "    # channels = np.tile(test_patch, (NUM_CLASSES, 1, 1))\n",
    "    # for (i, channel) in enumerate(channels):\n",
    "    #     channel = torch.as_tensor(channel).type(dtype=torch.float32)\n",
    "    #     channels[i] = torch.tensor(1.0).where(channel==i+1, torch.tensor(0.0))\n",
    "    # test_patch = torch.from_numpy(channels).to(DEVICE)\n",
    "    test_patch = torch.from_numpy(test_patch).to(DEVICE).unsqueeze(0)\n",
    "    print(test_patch.shape)\n",
    "    test_patch = test_patch.type(dtype=torch.float32)\n",
    "    predMask = unet(test_patch).squeeze()\n",
    "    predMask = torch.argmax(predMask, dim=0)\n",
    "    predMask = predMask.cpu().detach().numpy()\n",
    "    ax[3].imshow(predMask, cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the test dataset\n",
    "testDS = SegmentationDataset(wsi_names=test_img_names, mask_thumbnails=thumbnails_dict, pseudo_epoch_length=NUM_PSEUDO_EPOCHS)\n",
    "print(f\"[INFO] found {len(testDS)} samples in the test set...\")\n",
    "\n",
    "# create the test data loader\n",
    "testLoader = DataLoader(testDS, shuffle=True,\n",
    "\t                    batch_size=BATCH_SIZE, num_workers=4)\n",
    "\n",
    "# calculate steps per epoch for test set\n",
    "testSteps = len(testDS) // BATCH_SIZE\n",
    "\n",
    "# loop over epochs\n",
    "print(\"[INFO] testing the network...\")\n",
    "startTime = time.time()\n",
    "# initialize the total test loss\n",
    "totalTestLoss = 0\n",
    "# switch off autograd\n",
    "with torch.no_grad():\n",
    "    # set the model in evaluation mode\n",
    "    unet.eval()\n",
    "    for (i, (x, y)) in tqdm(enumerate(testLoader)):\n",
    "        # send the input to the device\n",
    "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "        # make the predictions and calculate the test loss\n",
    "        pred = unet(x).squeeze()\n",
    "        y = y.squeeze().type(dtype=torch.long)\n",
    "        totalTestLoss += lossFunc(pred, y)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            all_figure, all_ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 20))\n",
    "            all_ax[0].imshow(torch.as_tensor(x[i].cpu().detach().numpy()).permute(1, 2, 0), cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "            realMask = y[i].cpu().detach().numpy()\n",
    "            all_ax[1].imshow(realMask, cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "            predMask = torch.argmax(pred[i], dim=0)\n",
    "            predMask = predMask.cpu().detach().numpy()\n",
    "            all_ax[2].imshow(predMask, cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "            all_figure.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# calculate the average test loss\n",
    "avgTestLoss = (totalTestLoss / testSteps).cpu().detach().numpy()\n",
    "print(\"Test loss: {avgTestLoss}\")\n",
    "\n",
    "# display the total time needed to perform the testing\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to test the model: {:.2f}s\".format(endTime - startTime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de4631684bd2c882b6b797b2a828f7be340f598e9083880a321a8ea7c6fac874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
