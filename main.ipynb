{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated Gleason Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import openslide\n",
    "\n",
    "# Torch packages\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn import ConvTranspose2d\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import Module\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from focal_loss.focal_loss import FocalLoss\n",
    "\n",
    "# General packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Location of the training images\n",
    "DATA_PATH = '../../ganz/data/panda_dataset'\n",
    "\n",
    "# image and mask directories\n",
    "data_dir = f'{DATA_PATH}/train_images'\n",
    "mask_dir = f'{DATA_PATH}/train_label_masks'\n",
    "\n",
    "# Location of training labels\n",
    "train = pd.read_csv(f'{DATA_PATH}/train.csv').set_index('image_id')\n",
    "test = pd.read_csv(f'{DATA_PATH}/test.csv').set_index('image_id')\n",
    "submission = pd.read_csv(f'{DATA_PATH}/sample_submission.csv')\n",
    "\n",
    "# # Create training image name list\n",
    "# all_train_img_names = list(train.index)[:12]\n",
    "\n",
    "# Get mask thumbnail dictionary\n",
    "with open(\"./data/thumbnails.p\", \"rb\") as fp:\n",
    "    thumbnails_dict = pickle.load(fp)\n",
    "\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)\n",
    "# # determine if we will be pinning memory during data loading\n",
    "# PIN_MEMORY = True if DEVICE == \"cuda\" else False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_train_img_names = [\n",
    "#     '07a7ef0ba3bb0d6564a73f4f3e1c2293',\n",
    "#     '037504061b9fba71ef6e24c48c6df44d',\n",
    "#     '035b1edd3d1aeeffc77ce5d248a01a53',\n",
    "#     '059cbf902c5e42972587c8d17d49efed',\n",
    "#     '06a0cbd8fd6320ef1aa6f19342af2e68',\n",
    "#     '06eda4a6faca84e84a781fee2d5f47e1',\n",
    "#     '0a4b7a7499ed55c71033cefb0765e93d',\n",
    "#     '0838c82917cd9af681df249264d2769c',\n",
    "#     '046b35ae95374bfb48cdca8d7c83233f',\n",
    "#     '074c3e01525681a275a42282cd21cbde',\n",
    "#     '05abe25c883d508ecc15b6e857e59f32',\n",
    "#     '05f4e9415af9fdabc19109c980daf5ad',\n",
    "#     '060121a06476ef401d8a21d6567dee6d',\n",
    "#     '068b0e3be4c35ea983f77accf8351cc8',\n",
    "#     '08f055372c7b8a7e1df97c6586542ac8'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the number of channels in the input, number of classes,\n",
    "# and number of levels in the U-Net model\n",
    "NUM_CHANNELS = 1\n",
    "NUM_CLASSES = 1\n",
    "NUM_LEVELS = 3\n",
    "# initialize learning rate, number of epochs to train for, and the batch size\n",
    "INIT_LR = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "# define the input image dimensions\n",
    "PATCH_WIDTH = 256\n",
    "PATCH_HEIGHT = 256\n",
    "# define threshold to filter weak predictions\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# define the validation split\n",
    "VAL_SPLIT = 0.85\n",
    "\n",
    "# define the test split\n",
    "TEST_SPLIT = 0.95\n",
    "\n",
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"./output\"\n",
    "# define the path to the output serialized model, model training\n",
    "# plot, and testing image paths\n",
    "MODEL_PATH = f\"{BASE_OUTPUT}/unet_tgs_salt.pth\"\n",
    "PLOT_PATH = f\"{BASE_OUTPUT}/plot.png\"\n",
    "TEST_PATHS = f\"{BASE_OUTPUT}/test_paths.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentationDataset(Dataset):\n",
    "  def __init__(self, \n",
    "              wsi_names, \n",
    "              mask_thumbnails,\n",
    "              pseudo_epoch_length: int = 1024, \n",
    "              transformations = None):\n",
    "    self.wsi_names = wsi_names\n",
    "    self.mask_thumbnails = mask_thumbnails\n",
    "    self.pseudo_epoch_length = pseudo_epoch_length\n",
    "\n",
    "    # opens all slides and stores them in slide_dict\n",
    "    self.slide_dict = self.make_slide_dict(wsi_names=self.wsi_names)\n",
    "\n",
    "    # samples a list of patch coordinates and annotations \n",
    "    self.sample_dict = self.sample_coord_list(pseudo_epoch_length=self.pseudo_epoch_length)\n",
    "\n",
    "    if transformations is not None:\n",
    "      self.transformations = transformations\n",
    "    else:\n",
    "      self.transformations = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "  def make_slide_dict(self, wsi_names):\n",
    "    slide_dict = {}\n",
    "    bad_samples = []\n",
    "    for wsi_name in tqdm(wsi_names, total=len(wsi_names), desc='Make Slide Dict'):\n",
    "      if wsi_name not in slide_dict:\n",
    "        slide_path = os.path.join(data_dir, f'{wsi_name}.tiff')\n",
    "        mask_path = os.path.join(mask_dir, f'{wsi_name}_mask.tiff')\n",
    "        if os.path.exists(slide_path) and os.path.exists(mask_path):\n",
    "          slide_dict[wsi_name] = {}\n",
    "          slide_dict[wsi_name]['slide'] = openslide.OpenSlide(slide_path)\n",
    "          slide_dict[wsi_name]['mask'] = openslide.OpenSlide(mask_path)\n",
    "          slide_dict[wsi_name]['size'] = slide_dict[wsi_name]['slide'].dimensions\n",
    "        else:\n",
    "          bad_samples.append(wsi_name)\n",
    "    # print(bad_samples)\n",
    "    # print(len(bad_samples))\n",
    "    for wsi_name in bad_samples:\n",
    "      self.wsi_names.remove(wsi_name)\n",
    "    return slide_dict\n",
    "\n",
    "  def sample_coord_list(self, pseudo_epoch_length):\n",
    "    # # sample random coordinates\n",
    "    # filenames, coords = self._sample_random_coords(pseudo_epoch_length)\n",
    "\n",
    "    # sample nonempty coordinates\n",
    "    filenames, coords = self._sample_nonempty_coords(pseudo_epoch_length)\n",
    "    \n",
    "    # bring everything in one dict\n",
    "    sample_dict = {}\n",
    "    for index, (filename, coord) in enumerate(zip(filenames, coords)):\n",
    "      sample_dict[index] = {'filename': filename, 'coordinates': coord}\n",
    "\n",
    "    return sample_dict\n",
    "\n",
    "  def _sample_nonempty_coords(self, pseudo_epoch_length):\n",
    "    filenames = []\n",
    "    coords = []\n",
    "    for i in range(pseudo_epoch_length):\n",
    "      filename = random.choice(self.wsi_names, size=1)[0]\n",
    "      # mask_slide = self.slide_dict[filename]['mask']\n",
    "      width, height = self.slide_dict[filename]['size']\n",
    "      mask_thumbnail = self.mask_thumbnails[filename]\n",
    "      # thumbnail_width, thumbnail_height = mask_thumbnail.size()\n",
    "      indices = np.transpose(np.where(mask_thumbnail>0))\n",
    "      # print(indices)\n",
    "      # plt.figure(figsize=(10,10))\n",
    "      # plt.imshow(mask_thumbnail)\n",
    "      # plt.show()\n",
    "      rand_index = random.randint(len(indices))\n",
    "      coord = indices[rand_index]\n",
    "      # print(\"coord: \" + coord)\n",
    "      # coord = []\n",
    "      # only_background = True\n",
    "      # while only_background:\n",
    "      #   coord = list(random.randint(low=(0, 0), \n",
    "      #                               high=(thumbnail_width, thumbnail_height),\n",
    "      #                               size=2))\n",
    "      #   # print(mask_thumbnail[coord[0]][coord[1]])\n",
    "      #   if mask_thumbnail[coord[0]][coord[1]]>0:\n",
    "      #     only_background = False\n",
    "      coord[0] = coord[0]*PATCH_WIDTH + random.randint(low=-PATCH_WIDTH//2,\n",
    "                                                        high=PATCH_WIDTH//2)\n",
    "      if coord[0]<0: coord[0]=0\n",
    "      if coord[0]>width-PATCH_WIDTH: coord[0]=width-PATCH_WIDTH\n",
    "      coord[1] = coord[1]*PATCH_HEIGHT + random.randint(low=-PATCH_HEIGHT//2,\n",
    "                                                        high=PATCH_HEIGHT//2)\n",
    "      if coord[1]<0: coord[1]=0\n",
    "      if coord[1]>height-PATCH_HEIGHT: coord[1]=height-PATCH_HEIGHT\n",
    "      # print(\"Nonempty patch found\")\n",
    "      filenames.append(filename)\n",
    "      coords.append(coord)\n",
    "    return filenames, coords\n",
    "\n",
    "  def _sample_random_coords(self, pseudo_epoch_length):\n",
    "    filenames = list(random.choice(self.wsi_names, size=pseudo_epoch_length, replace=True))\n",
    "    coords = []\n",
    "    for filename in filenames:\n",
    "      width, height = self.slide_dict[filename]['size']\n",
    "      xy = list(random.randint(low=(0, 0), \n",
    "                              high=(width-PATCH_WIDTH, height-PATCH_HEIGHT),\n",
    "                              size=2))\n",
    "      coords.append(xy)\n",
    "    return filenames, coords\n",
    "\n",
    "  def __len__(self):\n",
    "    # return the number of total samples\n",
    "    return self.pseudo_epoch_length\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # grab the image from the current index\n",
    "    coords = self.sample_dict[index]['coordinates'].copy()\n",
    "    filename = self.sample_dict[index]['filename']\n",
    "\n",
    "    # load patch and mask\n",
    "    img, mask = self.load_image(filename, coords)\n",
    "\n",
    "    img = self.transformations(img)\n",
    "    mask = torch.as_tensor(mask) #.permute(1, 2, 0)\n",
    "\n",
    "    return img, mask\n",
    "\n",
    "  def load_image(self, filename, coords):\n",
    "    \"\"\"Loads an image and corresponding mask patch from a slide and returns it as a numpy array.\"\"\"\n",
    "    slide = self.slide_dict[filename]['slide']\n",
    "    patch = slide.read_region(coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0).convert('RGB')\n",
    "    mask_slide = self.slide_dict[filename]['mask']\n",
    "    mask_patch = mask_slide.read_region(coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0).convert('RGB')\n",
    "    # print(mask_patch.size)\n",
    "    # print(np.mean(mask_patch))\n",
    "    # print(np.asarray(mask_patch, dtype=np.uint8))\n",
    "    return np.asarray(patch, dtype=np.uint8), np.asarray(mask_patch, dtype=np.uint8)[:,:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNET Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(Module):\n",
    "\tdef __init__(self, inChannels, outChannels):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the convolution and RELU layers\n",
    "\t\tself.conv1 = Conv2d(inChannels, outChannels, 3)\n",
    "\t\tself.relu = ReLU()\n",
    "\t\tself.conv2 = Conv2d(outChannels, outChannels, 3)\n",
    "    \n",
    "\tdef forward(self, x):\n",
    "\t\t# apply CONV => RELU => CONV block to the inputs and return it\n",
    "\t\treturn self.conv2(self.relu(self.conv1(x)))\n",
    "\n",
    "class Encoder(Module):\n",
    "\tdef __init__(self, channels=(3, 16, 32, 64)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# store the encoder blocks and maxpooling layer\n",
    "\t\tself.encBlocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.pool = MaxPool2d(2)\n",
    "    \n",
    "\tdef forward(self, x):\n",
    "\t\t# initialize an empty list to store the intermediate outputs\n",
    "\t\tblockOutputs = []\n",
    "\t\t# loop through the encoder blocks\n",
    "\t\tfor block in self.encBlocks:\n",
    "\t\t\t# pass the inputs through the current encoder block, store\n",
    "\t\t\t# the outputs, and then apply maxpooling on the output\n",
    "\t\t\tx = block(x)\n",
    "\t\t\tblockOutputs.append(x)\n",
    "\t\t\tx = self.pool(x)\n",
    "\t\t# return the list containing the intermediate outputs\n",
    "\t\treturn blockOutputs\n",
    "\n",
    "class Decoder(Module):\n",
    "\tdef __init__(self, channels=(64, 32, 16)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the number of channels, upsampler blocks, and\n",
    "\t\t# decoder blocks\n",
    "\t\tself.channels = channels\n",
    "\t\tself.upconvs = ModuleList(\n",
    "\t\t\t[ConvTranspose2d(channels[i], channels[i + 1], 2, 2)\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\t\tself.dec_blocks = ModuleList(\n",
    "\t\t\t[Block(channels[i], channels[i + 1])\n",
    "\t\t\t \tfor i in range(len(channels) - 1)])\n",
    "\n",
    "\tdef forward(self, x, encFeatures):\n",
    "\t\t# loop through the number of channels\n",
    "\t\tfor i in range(len(self.channels) - 1):\n",
    "\t\t\t# pass the inputs through the upsampler blocks\n",
    "\t\t\tx = self.upconvs[i](x)\n",
    "\t\t\t# crop the current features from the encoder blocks,\n",
    "\t\t\t# concatenate them with the current upsampled features,\n",
    "\t\t\t# and pass the concatenated output through the current\n",
    "\t\t\t# decoder block\n",
    "\t\t\tencFeat = self.crop(encFeatures[i], x)\n",
    "\t\t\tx = torch.cat([x, encFeat], dim=1)\n",
    "\t\t\tx = self.dec_blocks[i](x)\n",
    "\t\t# return the final decoder output\n",
    "\t\treturn x\n",
    "    \n",
    "\tdef crop(self, encFeatures, x):\n",
    "\t\t# grab the dimensions of the inputs, and crop the encoder\n",
    "\t\t# features to match the dimensions\n",
    "\t\t# print(x.shape)\n",
    "\t\t(_, _, H, W) = x.shape\n",
    "\t\t# H = x.shape[-2]\n",
    "\t\t# W = x.shape[-1]\n",
    "\t\tencFeatures = transforms.CenterCrop([H, W])(encFeatures)\n",
    "\t\t# return the cropped features\n",
    "\t\treturn encFeatures\n",
    "\t\t\n",
    "class UNet(Module):\n",
    "\tdef __init__(self, encChannels=(3, 16, 32, 64),\n",
    "\t\t decChannels=(64, 32, 16),\n",
    "\t\t nbClasses=1, retainDim=True,\n",
    "\t\t outSize=(PATCH_WIDTH, PATCH_HEIGHT)):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# initialize the encoder and decoder\n",
    "\t\tself.encoder = Encoder(encChannels)\n",
    "\t\tself.decoder = Decoder(decChannels)\n",
    "\t\t# initialize the regression head and store the class variables\n",
    "\t\tself.head = Conv2d(decChannels[-1], nbClasses, 1)\n",
    "\t\tself.retainDim = retainDim\n",
    "\t\tself.outSize = outSize\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# grab the features from the encoder\n",
    "\t\tencFeatures = self.encoder(x)\n",
    "\t\t# pass the encoder features through decoder making sure that\n",
    "\t\t# their dimensions are suited for concatenation\n",
    "\t\tdecFeatures = self.decoder(encFeatures[::-1][0],\n",
    "\t\t\tencFeatures[::-1][1:])\n",
    "\t\t# pass the decoder features through the regression head to\n",
    "\t\t# obtain the segmentation mask\n",
    "\t\tmap = self.head(decFeatures)\n",
    "\t\t# check to see if we are retaining the original output\n",
    "\t\t# dimensions and if so, then resize the output to match them\n",
    "\t\tif self.retainDim:\n",
    "\t\t\tmap = F.interpolate(map, self.outSize)\n",
    "\t\t# return the segmentation map\n",
    "\t\treturn map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make Slide Dict: 100%|██████████| 4166/4166 [00:03<00:00, 1130.35it/s]\n",
      "Make Slide Dict: 100%|██████████| 736/736 [00:00<00:00, 1166.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] found 1024 samples in the training set...\n",
      "[INFO] found 1024 samples in the validation set...\n"
     ]
    }
   ],
   "source": [
    "# Take only radboud slides\n",
    "radboud = train.loc[train[\"data_provider\"]==\"radboud\"]\n",
    "\n",
    "# Take only wsi names\n",
    "all__img_names = list(radboud.index)\n",
    "\n",
    "# partition the data into training and testing splits using 95% of\n",
    "# the data for training and the remaining 5% for testing\n",
    "test_split_size = math.floor(TEST_SPLIT*len(all__img_names))\n",
    "test_split = torch.utils.data.random_split(all__img_names,\n",
    "\t\t\t\t\t\t\t\t\t[test_split_size, len(all__img_names)-test_split_size], \n",
    "\t\t\t\t\t\t\t\t\tgenerator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# unpack the data split\n",
    "(all_train_img_names, test_img_names) = test_split\n",
    "all_train_img_names = list(all_train_img_names)\n",
    "test_img_names = list(test_img_names)\n",
    "\n",
    "# partition the data into training and validation splits using 85% of\n",
    "# the data for training and the remaining 15% for validation\n",
    "split_size = math.floor(VAL_SPLIT*len(all_train_img_names))\n",
    "split = torch.utils.data.random_split(all_train_img_names,\n",
    "\t\t\t\t\t\t\t\t\t[split_size, len(all_train_img_names)-split_size], \n",
    "\t\t\t\t\t\t\t\t\tgenerator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# unpack the data split\n",
    "(train_img_names, val_img_names) = split\n",
    "train_img_names = list(train_img_names)\n",
    "val_img_names = list(val_img_names)\n",
    "# print(train_img_names[:10])\n",
    "# print(val_img_names[:10])\n",
    "\n",
    "# create the train and validation datasets\n",
    "trainDS = SegmentationDataset(wsi_names=train_img_names, mask_thumbnails=thumbnails_dict, pseudo_epoch_length=1024)\n",
    "valDS = SegmentationDataset(wsi_names=val_img_names, mask_thumbnails=thumbnails_dict, pseudo_epoch_length=1024)\n",
    "print(f\"[INFO] found {len(trainDS)} samples in the training set...\")\n",
    "print(f\"[INFO] found {len(valDS)} samples in the validation set...\")\n",
    "\n",
    "# create the training and validation data loaders\n",
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "\tbatch_size=BATCH_SIZE, num_workers=4)\n",
    "valLoader = DataLoader(valDS, shuffle=False,\n",
    "\tbatch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "#     in_channels=3, out_channels=1, init_features=32, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize our UNet model\n",
    "unet = UNet().to(DEVICE)\n",
    "# initialize loss function and optimizer\n",
    "lossFunc = FocalLoss() #dice loss, focal loss\n",
    "opt = Adam(unet.parameters(), lr=INIT_LR)\n",
    "# calculate steps per epoch for training and validation set\n",
    "trainSteps = len(trainDS) // BATCH_SIZE\n",
    "valSteps = len(valDS) // BATCH_SIZE\n",
    "# initialize a dictionary to store training history\n",
    "H = {\"train_loss\": [], \"val_loss\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training the network...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:12<01:54, 12.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 1/10\n",
      "Train loss: nan, Val loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:25<01:40, 12.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 2/10\n",
      "Train loss: nan, Val loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:37<01:28, 12.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 3/10\n",
      "Train loss: nan, Val loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:50<01:15, 12.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] EPOCH: 4/10\n",
      "Train loss: nan, Val loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:55<01:23, 13.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/danylenko/auto_gleason/main.ipynb Cell 20'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000019vscode-remote?line=16'>17</a>\u001b[0m \tmasks_ax[e,i]\u001b[39m.\u001b[39mimshow(y[\u001b[39m0\u001b[39m], cmap\u001b[39m=\u001b[39mcmap, interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m, vmin\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, vmax\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000019vscode-remote?line=17'>18</a>\u001b[0m \u001b[39m# send the input to the device\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000019vscode-remote?line=18'>19</a>\u001b[0m (x, y) \u001b[39m=\u001b[39m (x\u001b[39m.\u001b[39;49mto(DEVICE), y\u001b[39m.\u001b[39mto(DEVICE))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000019vscode-remote?line=19'>20</a>\u001b[0m \u001b[39m# perform a forward pass and calculate the training loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000019vscode-remote?line=20'>21</a>\u001b[0m \u001b[39m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000019vscode-remote?line=21'>22</a>\u001b[0m \u001b[39m# print(y.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/main.ipynb#ch0000019vscode-remote?line=22'>23</a>\u001b[0m pred \u001b[39m=\u001b[39m unet(x)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "masks_figure, masks_ax = plt.subplots(nrows=NUM_EPOCHS, ncols=3, figsize=(10, 10))\n",
    "cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n",
    "\n",
    "# loop over epochs\n",
    "print(\"[INFO] training the network...\")\n",
    "startTime = time.time()\n",
    "for e in tqdm(range(NUM_EPOCHS)):\n",
    "\t# set the model in training mode\n",
    "\tunet.train()\n",
    "\t# initialize the total training and validation loss\n",
    "\ttotalTrainLoss = 0\n",
    "\ttotalValLoss = 0\n",
    "\n",
    "\t# loop over the training set\n",
    "\tfor (i, (x, y)) in enumerate(trainLoader):\n",
    "\t\tif i<3:\n",
    "\t\t\tmasks_ax[e,i].imshow(y[0], cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\t\t# send the input to the device\n",
    "\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\t\t# perform a forward pass and calculate the training loss\n",
    "\t\t# print(x.shape)\n",
    "\t\t# print(y.shape)\n",
    "\t\tpred = unet(x)\n",
    "\t\ty = y[:,:1]\n",
    "\t\t# print(y.shape)\n",
    "\t\tloss = lossFunc(pred, y)\n",
    "\t\t# first, zero out any previously accumulated gradients, then\n",
    "\t\t# perform backpropagation, and then update model parameters\n",
    "\t\topt.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\topt.step()\n",
    "\t\t# add the loss to the total training loss so far\n",
    "\t\ttotalTrainLoss += loss\n",
    "    \n",
    "\t# switch off autograd\n",
    "\twith torch.no_grad():\n",
    "\t\t# set the model in evaluation mode\n",
    "\t\tunet.eval()\n",
    "\t\t# loop over the validation set\n",
    "\t\tfor (x, y) in valLoader:\n",
    "\t\t\t# send the input to the device\n",
    "\t\t\t(x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
    "\t\t\t# make the predictions and calculate the validation loss\n",
    "\t\t\tpred = unet(x)\n",
    "\t\t\ty = y[:,:1]\n",
    "\t\t\ttotalValLoss += lossFunc(pred, y)\n",
    "\n",
    "\t# calculate the average training and validation loss\n",
    "\tavgTrainLoss = totalTrainLoss / trainSteps\n",
    "\tavgValLoss = totalValLoss / valSteps\n",
    "\t# update our training history\n",
    "\tH[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
    "\tH[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
    "\t# print the model training and validation information\n",
    "\tprint(\"[INFO] EPOCH: {}/{}\".format(e + 1, NUM_EPOCHS))\n",
    "\tprint(\"Train loss: {:.6f}, Val loss: {:.4f}\".format(\n",
    "\t\tavgTrainLoss, avgValLoss))\n",
    "\n",
    "# display the total time needed to perform the training\n",
    "endTime = time.time()\n",
    "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(\n",
    "\tendTime - startTime))\n",
    "\n",
    "masks_figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training loss\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(PLOT_PATH)\n",
    "# serialize the model to disk\n",
    "torch.save(unet, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n",
    "cmap = matplotlib.colors.ListedColormap(['black', 'gray', 'green', 'yellow', 'orange', 'red'])\n",
    "\n",
    "test_img_name = test_img_names[1]\n",
    "print(test_img_name)\n",
    "\n",
    "test_slide_path = os.path.join(data_dir, f'{test_img_name}.tiff')\n",
    "test_slide = openslide.OpenSlide(test_slide_path)\n",
    "test_slide_size = test_slide.dimensions\n",
    "print(test_slide_size)\n",
    "thumbnail = test_slide.get_thumbnail((test_slide_size[0]/PATCH_WIDTH, test_slide_size[1]/PATCH_HEIGHT)).convert('RGB')\n",
    "ax[0].imshow(thumbnail)\n",
    "\n",
    "test_coords = [57*PATCH_WIDTH,28*PATCH_HEIGHT]\n",
    "test_patch = test_slide.read_region(test_coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0).convert('RGB')\n",
    "test_patch = np.asarray(test_patch, dtype=np.uint8)\n",
    "ax[1].imshow(test_patch)\n",
    "\n",
    "test_mask_path = os.path.join(mask_dir, f'{test_img_name}_mask.tiff')\n",
    "test_mask = openslide.OpenSlide(test_mask_path)\n",
    "test_mask_patch = test_mask.read_region(test_coords, size=(PATCH_WIDTH, PATCH_HEIGHT), level=0).convert('RGB')[:,:,0]\n",
    "test_mask_patch = np.asarray(test_mask_patch, dtype=np.uint8)\n",
    "ax[2].imshow(test_mask_patch, cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\n",
    "unet.eval()\n",
    "with torch.no_grad():\n",
    "    transformations = transforms.Compose([transforms.ToTensor()])\n",
    "    print(test_patch.shape)\n",
    "    test_patch = np.transpose(test_patch, (2,0,1))\n",
    "    test_patch = np.expand_dims(test_patch, 0)\n",
    "    # test_patch = transformations(test_patch)\n",
    "    print(test_patch.shape)\n",
    "    test_patch = torch.from_numpy(test_patch).to(DEVICE)\n",
    "    test_patch = test_patch.type(dtype=torch.float32)\n",
    "    predMask = unet(test_patch).squeeze()\n",
    "    predMask = torch.sigmoid(predMask)\n",
    "    predMask = predMask.cpu().numpy()\n",
    "    ax[3].imshow(predMask[:,:], cmap=cmap, interpolation='nearest', vmin=0, vmax=5)\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de4631684bd2c882b6b797b2a828f7be340f598e9083880a321a8ea7c6fac874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
