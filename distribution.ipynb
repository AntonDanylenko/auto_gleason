{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.load import *\n",
    "from lib.dataset import *\n",
    "from lib.thumbnail import *\n",
    "from lib.globals import *\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_train_img_names, test_img_names = load()\n",
    "\n",
    "# Get mask thumbnail dictionary\n",
    "thumbnail_filename = \"./data/thumbnails_\" + str(PATCH_WIDTH) + \"x\" + str(PATCH_HEIGHT) + \".p\"\n",
    "if not os.path.exists(thumbnail_filename):\n",
    "    create_thumbnails(PATCH_WIDTH, PATCH_HEIGHT)\n",
    "with open(thumbnail_filename, \"rb\") as fp:\n",
    "    thumbnails_dict = pickle.load(fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Make Slide Dict: 100%|██████████| 4386/4386 [00:03<00:00, 1107.17it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'8a539748fa4ea8902c3471f43911b7dd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/home/danylenko/auto_gleason/distribution.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/distribution.ipynb#ch0000003vscode-remote?line=16'>17</a>\u001b[0m val_img_names \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(val_img_names)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/distribution.ipynb#ch0000003vscode-remote?line=17'>18</a>\u001b[0m \u001b[39m# print(train_img_names[:10])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/distribution.ipynb#ch0000003vscode-remote?line=18'>19</a>\u001b[0m \u001b[39m# print(val_img_names[:10])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/distribution.ipynb#ch0000003vscode-remote?line=19'>20</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/distribution.ipynb#ch0000003vscode-remote?line=20'>21</a>\u001b[0m \u001b[39m# create the train and validation datasets\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/distribution.ipynb#ch0000003vscode-remote?line=21'>22</a>\u001b[0m trainDS \u001b[39m=\u001b[39m SegmentationDataset(wsi_names\u001b[39m=\u001b[39;49mtrain_img_names, mask_thumbnails\u001b[39m=\u001b[39;49mthumbnails_dict, pseudo_epoch_length\u001b[39m=\u001b[39;49mNUM_PSEUDO_EPOCHS)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/distribution.ipynb#ch0000003vscode-remote?line=22'>23</a>\u001b[0m valDS \u001b[39m=\u001b[39m SegmentationDataset(wsi_names\u001b[39m=\u001b[39mval_img_names, mask_thumbnails\u001b[39m=\u001b[39mthumbnails_dict, pseudo_epoch_length\u001b[39m=\u001b[39mNUM_PSEUDO_EPOCHS)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu/home/danylenko/auto_gleason/distribution.ipynb#ch0000003vscode-remote?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[INFO] found \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(trainDS)\u001b[39m}\u001b[39;00m\u001b[39m samples in the training set...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/auto_gleason/lib/dataset.py:26\u001b[0m, in \u001b[0;36mSegmentationDataset.__init__\u001b[0;34m(self, wsi_names, mask_thumbnails, pseudo_epoch_length, transformations)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslide_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_slide_dict(wsi_names\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwsi_names)\n\u001b[1;32m     25\u001b[0m \u001b[39m# samples a list of patch coordinates and annotations \u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample_coord_list(pseudo_epoch_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpseudo_epoch_length)\n\u001b[1;32m     28\u001b[0m \u001b[39mif\u001b[39;00m transformations \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformations \u001b[39m=\u001b[39m transformations\n",
      "File \u001b[0;32m~/auto_gleason/lib/dataset.py:58\u001b[0m, in \u001b[0;36mSegmentationDataset.sample_coord_list\u001b[0;34m(self, pseudo_epoch_length)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_coord_list\u001b[39m(\u001b[39mself\u001b[39m, pseudo_epoch_length):\n\u001b[1;32m     54\u001b[0m   \u001b[39m# # sample random coordinates\u001b[39;00m\n\u001b[1;32m     55\u001b[0m   \u001b[39m# filenames, coords = self._sample_random_coords(pseudo_epoch_length)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \n\u001b[1;32m     57\u001b[0m   \u001b[39m# sample nonempty coordinates\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m   filenames, coords \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sample_nonempty_coords(pseudo_epoch_length)\n\u001b[1;32m     60\u001b[0m   \u001b[39m# bring everything in one dict\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   sample_dict \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m~/auto_gleason/lib/dataset.py:76\u001b[0m, in \u001b[0;36mSegmentationDataset._sample_nonempty_coords\u001b[0;34m(self, pseudo_epoch_length)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m# mask_slide = self.slide_dict[filename]['mask']\u001b[39;00m\n\u001b[1;32m     75\u001b[0m width, height \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslide_dict[filename][\u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 76\u001b[0m mask_thumbnail \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmask_thumbnails[filename]\n\u001b[1;32m     77\u001b[0m indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtranspose(np\u001b[39m.\u001b[39mwhere(mask_thumbnail\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[1;32m     78\u001b[0m \u001b[39m# print(indices.size)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: '8a539748fa4ea8902c3471f43911b7dd'"
     ]
    }
   ],
   "source": [
    "\n",
    "# determine the device to be used for training and evaluation\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# print(DEVICE)\n",
    "# # determine if we will be pinning memory during data loading\n",
    "# PIN_MEMORY = True if DEVICE == \"cuda\" else False\n",
    "\n",
    "# partition the data into training and validation splits using 85% of\n",
    "# the data for training and the remaining 15% for validation\n",
    "split_size = math.floor(VAL_SPLIT*len(all_train_img_names))\n",
    "split = torch.utils.data.random_split(all_train_img_names,\n",
    "                                    [split_size, len(all_train_img_names)-split_size], \n",
    "                                    generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# unpack the data split\n",
    "(train_img_names, val_img_names) = split\n",
    "train_img_names = list(train_img_names)\n",
    "val_img_names = list(val_img_names)\n",
    "# print(train_img_names[:10])\n",
    "# print(val_img_names[:10])\n",
    "\n",
    "# create the train and validation datasets\n",
    "trainDS = SegmentationDataset(wsi_names=train_img_names, mask_thumbnails=thumbnails_dict, pseudo_epoch_length=NUM_PSEUDO_EPOCHS)\n",
    "valDS = SegmentationDataset(wsi_names=val_img_names, mask_thumbnails=thumbnails_dict, pseudo_epoch_length=NUM_PSEUDO_EPOCHS)\n",
    "print(f\"[INFO] found {len(trainDS)} samples in the training set...\")\n",
    "print(f\"[INFO] found {len(valDS)} samples in the validation set...\")\n",
    "\n",
    "# create the training and validation data loaders\n",
    "trainLoader = DataLoader(trainDS, shuffle=True,\n",
    "    batch_size=BATCH_SIZE, num_workers=4)\n",
    "valLoader = DataLoader(valDS, shuffle=False,\n",
    "    batch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "counts = [0,0,0,0,0,0]\n",
    "for e in tqdm(range(1)):\n",
    "    # loop over the training set\n",
    "    for (x, y) in trainLoader[:2]:\n",
    "        for i in range(len(counts)):\n",
    "            counts[i] += y.count(i)\n",
    "\n",
    "figure, ax = plt.subplots(1,1,figsize=(30,30))\n",
    "x = np.arange(len(counts))\n",
    "\n",
    "ax[0].bar(x,counts)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de4631684bd2c882b6b797b2a828f7be340f598e9083880a321a8ea7c6fac874"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
